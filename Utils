import os
import glob
import numpy as np



# Opens mails in path folder and returns them as a string
# The function concatenates all mails in path folder into a string

def get_mails(path):
    content = ''
    for filename in glob.glob(os.path.join(path, '*.txt')):
        f = open(filename, 'r')
        content += f.read().replace('\n', '')
    return content



# Counts the number of mails in path folder - useful for estimation of priors

def no_mails(path):
    mails = 0
    for filename in glob.glob(os.path.join(path, '*.txt')):
        mails += 1
    return mails



# Iterates through the training set and returns a dictionary with
# each word in the training set and the number of occurrences.

def word_dict(content):
    word_count = dict()
    for word in content.split():
        word = word.lower()
        if word not in word_count:
            word_count[word] = 1
        else:
            word_count[word] += 1
    return word_count



"""
Iterates through the word/occurrence dictionary and assigns
a value between 0 and 1 to each value in the dictionary.
In order for the program to run faster and ignore rare words, which are
likely to be the result of typos or punctuation inconsistencies, I chose to only
include the words that occur at least 10 times in the training set.
"""

def normalize_values(dct):
    normalized = dict()
    total = 0
    for key in dct:
        if dct[key] >= 10:
            total += dct[key]
    for key in dct:
        if dct[key] >= 10:
            value = float(dct[key])
            norm = value / total
            normalized[key] = norm
    return normalized



# This function computes the prior probability of ham/spam based on the dataset.
# It takes as arguments the folder paths to the ham and spam e-mails.

def compute_prior(ham_path, spam_path):
    n_ham = float(no_mails(ham_path))
    n_spam = float(no_mails(spam_path))
    prior = (n_ham / (n_ham + n_spam))
    return prior



"""
Compute probability of a mail being spam/ham.
The prior was chosen to be 0.5 in order for the algorithm to not be biased.
In 2017 it was estimated that roughly 56% of e-mails sent are spam.
Source: https://www.statista.com/statistics/420391/spam-email-traffic-share/
If you'd like to use different priors, you can either manually change the parameter inside the 
compute_probability function, or set it to be equal to the result of the function compute_prior above.
The latter will compute priors that are biased in favour of the training data.
"""

def compute_probability(ham_dict,spam_dict, mail, prior = 0.5):
    h_probability = 1 * prior
    s_probability = 1 * (1 - prior)
    for word in ham_dict.keys():
        if word in mail.split():
            h_probability *= ham_dict[word]
        else:
            h_probability *= (1 - ham_dict[word])
    for word in spam_dict.keys():
        if word in mail.split():
            s_probability *= spam_dict[word]
        else:
            s_probability *= (1 - spam_dict[word])
    if h_probability == 1 or s_probability == 1:
        print("None of the words in the e-mail were found in the vocabulary")
        return None
    probability = (h_probability) / (h_probability + s_probability)
    print(probability)
    return probability



"""Having more words in the vocabulary leads to longer multiplications of probabilities, which
quickly results in underflow. Because of it, I implemented a scoring method which uses
sums of logs as opposed to multiplication of probabilities.
For this reason, in testing the algorithm I used the function compute_score as opposed to compute_probability.
"""

def compute_score(ham_dict, spam_dict, mail, prior = 0.5):
    h_score = 0
    s_score = 0
    for word in ham_dict.keys():
        if word in mail.split():
            h_score -= np.log(ham_dict[word])
        else:
            h_score -= np.log(1 - (ham_dict[word]))
    for word in spam_dict.keys():
        if word in mail.split():
            s_score -= np.log(spam_dict[word])
        else:
            s_score -= np.log((1 - spam_dict[word]))
    if h_score == 0 or s_score == 0:
        print("None of the words in the e-mail were found in the vocabulary")
        return None
    probability = (h_score * prior) / (h_score * prior + s_score * (1 - prior))
    return probability



""" This function brings everything together and returns the accuracy of the algorithm.
The first input is the e-mail saved as a string. The second and third parameters are the outputs of
the normalize_values function for ham and spam.
The 0.5 is the threshold for the algorithm to classify something as spam/ham (i.e. if it is more than 50% sure
an e-mail is spam, it will classify it as spam)
"""

def classify(mail, ham_norm, spam_norm):
    content = mail.replace('\n','')
    score = compute_score(ham_norm, spam_norm, content)
    if score > 0.5:
        return "ham"
    else:
        return "spam"



"""This function does the same thing as classify, except it is used to classify multiple e-mails
from a folder, provided that the e-mails are already sorted into spam and ham folders.
It takes as input the path to the folder, ham_norm and spam_norm as above, as well as
the label of the e-mails in the folder. The label var can be set to either "spam" or "ham", depending
on the type of e-mails in the folder.
"""

def get_results(path, ham_norm, spam_norm, var = ''):
    misclassified = 0
    correct = 0
    no_testdata = no_mails(path)
    for filename in glob.glob(os.path.join(path, '*.txt')):
        f = open(filename, 'r')
        content = f.read().replace('\n','')
        score = compute_score(ham_norm, spam_norm, content)
        if var == 'spam':
            if score > 0.5:
                misclassified += 1.0
            else:
                correct += 1.0
        if var == 'ham':
            if score < 0.5:
                misclassified += 1.0
            else:
                correct += 1.0
        print(correct, correct + misclassified, "/", no_testdata)
    accuracy = float(correct / (misclassified + correct))
    return accuracy







