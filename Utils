import os
import glob
import numpy as np



# Opens mails in path folder and returns them as a string

def get_mails(path):
    content = ''
    for filename in glob.glob(os.path.join(path, '*.txt')):
        f = open(filename, 'r')
        content += f.read().replace('\n', '')
    return content



# Counts the number of mails in path folder - useful for estimation of priors

def no_mails(path):
    mails = 0
    for filename in glob.glob(os.path.join(path, '*.txt')):
        mails += 1
    return mails



# Iterates through the training set of e-mails saved as a string
# and returns a dictionary with each word in the training set and the number of occurrences

def word_dict(content):
    word_count = dict()
    for word in content.split():
        word = word.lower()
        if word not in word_count:
            word_count[word] = 1
        else:
            word_count[word] += 1
    return word_count



# Iterates through the word/occurrence dictionary and assigns a value between 0 and 1
# to each value in the dictionary

def normalize_values(dct):
    normalized = dict()
    total = 0
    for key in dct:
        total += dct[key]
    for key in dct:
        value = float(dct[key])
        norm = value / total
        normalized[key] = norm
    return normalized



"""
Compute probability of a mail being spam/ham.
The prior was chosen to be 0.5 in order for the algorithm to not be biased.
In 2017 it was estimated that roughly 56% of e-mails sent are spam.
Source: https://www.statista.com/statistics/420391/spam-email-traffic-share/
"""

def compute_probability(ham_dict,spam_dict, mail, prior = 0.5):
    h_probability = 1 * prior
    s_probability = 1 * (1-prior)
    for word in ham_dict.keys():
        if word in mail.split():
            h_probability *= ham_dict[word]
        else:
            h_probability *= (1 - ham_dict[word])
    for word in spam_dict.keys():
        if word in mail.split():
            s_probability *= spam_dict[word]
        else:
            s_probability *= (1 - spam_dict[word])
    probability = (h_probability) / (h_probability + s_probability)
    return probability



"""Having more words in the vocabulary leads to longer multiplications of probabilities, which
quickly results in underflow. Because of it, I implemented a scoring method which uses
sums of logs as opposed to multiplication of probabilities.
"""

def compute_score(dct, mail):
    score = 0
    for word in mail.split():
        if word in dct.keys():
            score -= np.log(dct[word])
    return score





